"""
MaxSpeeding Meta Ads æ™ºèƒ½æ—¥æŠ¥ç³»ç»Ÿ - Benchmark å¯¹æ¯”åˆ†æž

ä¸Žè¡Œä¸šæ ‡å‡†å¯¹æ¯”ï¼Œè¯„ä¼°å¹¿å‘Šè¡¨çŽ°
"""
from typing import Dict, List, Optional
from loguru import logger
from config.benchmarks import BenchmarkManager


class BenchmarkAnalyzer:
    """Benchmark åˆ†æžå™¨"""

    def __init__(self, custom_benchmarks: Optional[Dict] = None):
        """
        åˆå§‹åŒ–åˆ†æžå™¨

        Args:
            custom_benchmarks: è‡ªå®šä¹‰åŸºå‡†æ•°æ®
        """
        self.benchmark_manager = BenchmarkManager(custom_benchmarks)

    def analyze_all_metrics(self, data: Dict) -> Dict:
        """
        åˆ†æžæ‰€æœ‰æŒ‡æ ‡ä¸ŽåŸºå‡†çš„å¯¹æ¯”

        Args:
            data: å¹¿å‘Šæ•°æ®

        Returns:
            å„æŒ‡æ ‡çš„å¯¹æ¯”ç»“æžœ
        """
        results = {}

        # æ˜ å°„æ•°æ®å­—æ®µåˆ°åŸºå‡†æŒ‡æ ‡åç§°
        metric_mapping = {
            'roas': 'ROAS',
            'cpc': 'CPC',
            'ctr': 'CTR',
            'cpa': 'CPA',
            'frequency': 'Frequency',
            'conversion_rate': 'ConversionRate'
        }

        for data_field, benchmark_key in metric_mapping.items():
            value = data.get(data_field, 0)
            evaluation = self.benchmark_manager.evaluate(benchmark_key, value)

            results[benchmark_key] = {
                'value': round(value, 2),
                'benchmark_range': evaluation['industry_range'],
                'excellent': evaluation['excellent'],
                'status': evaluation['status'],
                'rating': evaluation['rating'],
                'gap_percent': evaluation['gap_percent'],
                'is_alert': evaluation['is_alert']
            }

        return results

    def get_overall_score(self, evaluation: Dict) -> float:
        """
        è®¡ç®—æ•´ä½“è¡¨çŽ°è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰

        Args:
            evaluation: æŒ‡æ ‡è¯„ä¼°ç»“æžœ

        Returns:
            æ•´ä½“è¯„åˆ†
        """
        if not evaluation:
            return 0

        scores = []

        for metric, result in evaluation.items():
            status = result.get('status', '')

            # æ ¹æ®çŠ¶æ€è¯„åˆ†
            if status == 'excellent':
                score = 100
            elif status == 'good':
                score = 75
            elif status == 'warning':
                score = 50
            elif status == 'critical':
                score = 25
            else:
                score = 50

            scores.append(score)

        return round(sum(scores) / len(scores), 1) if scores else 0

    def generate_benchmark_insight(self, evaluation: Dict) -> List[str]:
        """
        ç”Ÿæˆ Benchmark å¯¹æ¯”åˆ†æžæ´žå¯Ÿ

        Args:
            evaluation: æŒ‡æ ‡è¯„ä¼°ç»“æžœ

        Returns:
            æ´žå¯Ÿåˆ—è¡¨
        """
        insights = []

        for metric, result in evaluation.items():
            status = result['status']
            value = result['value']
            gap = result['gap_percent']

            if status == 'excellent':
                insights.append(f"âœ… {metric}: {value} ä¼˜äºŽè¡Œä¸šæ ‡å‡† {abs(gap)}%")
            elif status == 'good':
                insights.append(f"âœ… {metric}: {value} ç¬¦åˆè¡Œä¸šæ ‡å‡†")
            elif status == 'warning':
                if result.get('rating') == 'âš ï¸':
                    insights.append(f"âš ï¸ {metric}: {value} ç•¥ä½ŽäºŽè¡Œä¸šå‡å€¼")
            elif status == 'critical':
                insights.append(f"ðŸš¨ {metric}: {value} ä½ŽäºŽè¡Œä¸šæ ‡å‡†ï¼Œéœ€é‡ç‚¹å…³æ³¨")

        return insights

    def compare_with_previous(
        self,
        current_evaluation: Dict,
        previous_evaluation: Dict
    ) -> Dict:
        """
        å¯¹æ¯”å½“å‰ä¸Žä¸ŠæœŸçš„ Benchmark è¡¨çŽ°

        Args:
            current_evaluation: å½“å‰è¯„ä¼°
            previous_evaluation: ä¸ŠæœŸè¯„ä¼°

        Returns:
            å¯¹æ¯”ç»“æžœ
        """
        comparison = {}

        for metric in current_evaluation.keys():
            current_status = current_evaluation[metric]['status']
            previous_status = previous_evaluation.get(metric, {}).get('status', '')

            # çŠ¶æ€å˜åŒ–åˆ¤æ–­
            status_change = self._get_status_change(previous_status, current_status)

            comparison[metric] = {
                'current_status': current_status,
                'previous_status': previous_status,
                'change': status_change,
                'improved': status_change == 'improved',
                'declined': status_change == 'declined'
            }

        return comparison

    def _get_status_change(self, previous: str, current: str) -> str:
        """
        åˆ¤æ–­çŠ¶æ€å˜åŒ–

        Args:
            previous: ä¸ŠæœŸçŠ¶æ€
            current: å½“å‰çŠ¶æ€

        Returns:
            å˜åŒ–ç±»åž‹ (improved, declined, stable)
        """
        status_order = ['critical', 'warning', 'good', 'excellent']

        try:
            prev_index = status_order.index(previous) if previous else 1
            curr_index = status_order.index(current) if current else 1

            if curr_index > prev_index:
                return 'improved'
            elif curr_index < prev_index:
                return 'declined'
            else:
                return 'stable'
        except ValueError:
            return 'stable'
